{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GERaaD2VqmEb"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os\n",
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3yfmUqsRseaH"
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = 'C:/Users/suman/Downloads/images/train'\n",
    "validation_data_dir = 'C:/Users/suman/Downloads/images/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG7FOBAMtZz4",
    "outputId": "68c3ea20-e623-449a-ae73-be2a82de3c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhhdyPZTwBMw"
   },
   "outputs": [],
   "source": [
    "# num_classes = 5\n",
    "# img_rows,img_cols = 48,48\n",
    "# batch_size = 32\n",
    "\n",
    "# train_data_dir = '/Users/durgeshthakur/Deep Learning Stuff/Emotion Classification/fer2013/train'\n",
    "# validation_data_dir = '/Users/durgeshthakur/Deep Learning Stuff/Emotion Classification/fer2013/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nNPsYlh-wEh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24282 images belonging to 5 classes.\n",
      "Found 5937 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "\t\t\t\t\trescale=1./255,\n",
    "\t\t\t\t\trotation_range=30,\n",
    "\t\t\t\t\tshear_range=0.3,\n",
    "\t\t\t\t\tzoom_range=0.3,\n",
    "\t\t\t\t\twidth_shift_range=0.4,\n",
    "\t\t\t\t\theight_shift_range=0.4,\n",
    "\t\t\t\t\thorizontal_flip=True,\n",
    "\t\t\t\t\tfill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "\t\t\t\t\ttrain_data_dir,\n",
    "\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\tshuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\t\t\t\t\t\t\tvalidation_data_dir,\n",
    "\t\t\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\t\t\tshuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dm9PAwg1wElm"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "llJ2VdSWwEpN"
   },
   "outputs": [],
   "source": [
    "# Block-1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JfLNIn-LwEtr"
   },
   "outputs": [],
   "source": [
    "# Block-2 \n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Cc-6qNadwUQf"
   },
   "outputs": [],
   "source": [
    "# Block-3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SF8u9pwIwUTr"
   },
   "outputs": [],
   "source": [
    "# Block-4 \n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6OrhmVtrwUW4"
   },
   "outputs": [],
   "source": [
    "# Block-5\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h_Ho-K4PwUaA"
   },
   "outputs": [],
   "source": [
    "# Block-6\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LiehLQ05wUmd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,037\n",
      "Trainable params: 1,325,861\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Block-7\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QY5YiM8xwUpj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('Emotion_little_vgg.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6TnsUn6IwUst"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "755/755 [==============================] - 527s 693ms/step - loss: 2.1226 - accuracy: 0.2223 - val_loss: 1.5537 - val_accuracy: 0.3004\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55368, saving model to Emotion_little_vgg.h5\n",
      "Epoch 2/25\n",
      "755/755 [==============================] - 489s 648ms/step - loss: 1.5976 - accuracy: 0.2638 - val_loss: 1.5351 - val_accuracy: 0.2957\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55368 to 1.53515, saving model to Emotion_little_vgg.h5\n",
      "Epoch 3/25\n",
      "755/755 [==============================] - 472s 626ms/step - loss: 1.5500 - accuracy: 0.2992 - val_loss: 1.4711 - val_accuracy: 0.3663\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53515 to 1.47106, saving model to Emotion_little_vgg.h5\n",
      "Epoch 4/25\n",
      "755/755 [==============================] - 448s 593ms/step - loss: 1.5238 - accuracy: 0.3159 - val_loss: 1.4299 - val_accuracy: 0.3760\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47106 to 1.42988, saving model to Emotion_little_vgg.h5\n",
      "Epoch 5/25\n",
      "755/755 [==============================] - 425s 563ms/step - loss: 1.4573 - accuracy: 0.3628 - val_loss: 1.4743 - val_accuracy: 0.4113\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42988\n",
      "Epoch 6/25\n",
      "755/755 [==============================] - 407s 538ms/step - loss: 1.3687 - accuracy: 0.4204 - val_loss: 1.1898 - val_accuracy: 0.5134\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.42988 to 1.18985, saving model to Emotion_little_vgg.h5\n",
      "Epoch 7/25\n",
      "755/755 [==============================] - 366s 485ms/step - loss: 1.2857 - accuracy: 0.4619 - val_loss: 1.0016 - val_accuracy: 0.5857\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.18985 to 1.00161, saving model to Emotion_little_vgg.h5\n",
      "Epoch 8/25\n",
      "755/755 [==============================] - 399s 529ms/step - loss: 1.2136 - accuracy: 0.4997 - val_loss: 0.9423 - val_accuracy: 0.6179\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00161 to 0.94225, saving model to Emotion_little_vgg.h5\n",
      "Epoch 9/25\n",
      "755/755 [==============================] - 504s 668ms/step - loss: 1.1890 - accuracy: 0.5240 - val_loss: 0.9160 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94225 to 0.91598, saving model to Emotion_little_vgg.h5\n",
      "Epoch 10/25\n",
      "755/755 [==============================] - 442s 586ms/step - loss: 1.1616 - accuracy: 0.5296 - val_loss: 0.8710 - val_accuracy: 0.6546\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.91598 to 0.87101, saving model to Emotion_little_vgg.h5\n",
      "Epoch 11/25\n",
      "755/755 [==============================] - 498s 660ms/step - loss: 1.1225 - accuracy: 0.5466 - val_loss: 0.8900 - val_accuracy: 0.6552\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.87101\n",
      "Epoch 12/25\n",
      "755/755 [==============================] - 459s 607ms/step - loss: 1.1108 - accuracy: 0.5520 - val_loss: 0.8770 - val_accuracy: 0.6475\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.87101\n",
      "Epoch 13/25\n",
      "755/755 [==============================] - 465s 616ms/step - loss: 1.1060 - accuracy: 0.5595 - val_loss: 0.8604 - val_accuracy: 0.6663\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.87101 to 0.86040, saving model to Emotion_little_vgg.h5\n",
      "Epoch 14/25\n",
      "755/755 [==============================] - 459s 608ms/step - loss: 1.0713 - accuracy: 0.5769 - val_loss: 0.9210 - val_accuracy: 0.6502\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.86040\n",
      "Epoch 15/25\n",
      "755/755 [==============================] - 463s 613ms/step - loss: 1.0588 - accuracy: 0.5866 - val_loss: 0.8454 - val_accuracy: 0.6801\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.86040 to 0.84535, saving model to Emotion_little_vgg.h5\n",
      "Epoch 16/25\n",
      "755/755 [==============================] - 446s 591ms/step - loss: 1.0450 - accuracy: 0.5949 - val_loss: 0.7866 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.84535 to 0.78664, saving model to Emotion_little_vgg.h5\n",
      "Epoch 17/25\n",
      "755/755 [==============================] - 468s 620ms/step - loss: 1.0310 - accuracy: 0.5960 - val_loss: 0.8247 - val_accuracy: 0.6825\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.78664\n",
      "Epoch 18/25\n",
      "755/755 [==============================] - 522s 692ms/step - loss: 1.0346 - accuracy: 0.5975 - val_loss: 0.8774 - val_accuracy: 0.6653\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.78664\n",
      "Epoch 19/25\n",
      "755/755 [==============================] - 586s 776ms/step - loss: 1.0213 - accuracy: 0.6064 - val_loss: 0.8038 - val_accuracy: 0.6818\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.78664\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "epochs=25\n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RHuouiHEwUvm"
   },
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\suman\\Downloads\\haarcascade_frontalface_default.xml')\n",
    "classifier =load_model(r'C:\\Users\\suman\\Downloads\\Emotion_little_vgg.h5')\n",
    "\n",
    "class_labels = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-XbmADYPwUy4"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "    # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            label=class_labels[preds.argmax()]\n",
    "            label_position = (x,y)\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2wn_Y2QQwU4k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
